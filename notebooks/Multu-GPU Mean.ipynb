{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Mean Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "60770839-8745-4b6d-8967-015bade0ce45"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "import cudf, dask_cudf\n",
    "from dask_cuml import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple ways to get data into cuml, which will need to be tested:\n",
    "1. A large cudf object could be created and then passed to dask_cudf\n",
    "2. The workers are asked to fetch the data directly\n",
    "\n",
    "Since this will likely be running in a single worker per GPU mode, it will be important that the cuDF's are able to work across the GPUs (e.g. When a very large cuDF is partitioned across the workers- it will be important that the GPU memory is re-allocated to the new worker's local device and de-allocated on the cuDF's old device.)\n",
    "\n",
    "__Example workflow__:\n",
    "- User allocates a dask_cudf (or, eventually, a dask_cuml_array) and distributes it across the cluster\n",
    "- User calls MGMean().calculate(dask_cudf) after the dask_cudf\n",
    "- MGMean performs redistribution / preprocessing\n",
    "- MGMean gathers allocations (hostname/device/key triplets) from Dask workers\n",
    "- MGMean c++ code is executed with the allocation information as its argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "ef9fe3a0-56dd-4152-9d7a-17230e76f9f2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/conda/cuml/lib/python3.5/site-packages/distributed/bokeh/core.py:57: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn('\\n' + msg)\n"
     ]
    }
   ],
   "source": [
    "client = Client(n_workers=1, threads_per_worker = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "5d005f81-ffde-4df1-a9ab-61181f2c6748"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:41318\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>1</li>\n",
       "  <li><b>Memory: </b>1.57 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:41318' processes=1 cores=1>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "ed6e558e-bd21-4c4c-88c9-334ab797089a"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = cudf.DataFrame([('a', np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]).astype(np.float32)), ('b', np.array([2.0, 3.0, 4.0, 5.0, 6.0, 7.0]).astype(np.float32))])\n",
    "dask_df = dask_cudf.from_cudf(df, chunksize = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the Dataframe to scatter it out to the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "ce754eb3-c0ab-4a4c-af3d-0af6d4b0b42f"
    }
   },
   "outputs": [],
   "source": [
    "dask_df = client.persist(dask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "62668077-c270-462e-9d72-8afd66f3fc46"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dask_cudf.DataFrame | 3 tasks | 3 npartitions>\n"
     ]
    }
   ],
   "source": [
    "print(str(dask_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "058fd901-c613-4d64-88cb-fccc3181a269"
    }
   },
   "outputs": [],
   "source": [
    "m = mean.MGMean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "195e4b9f-5a4a-4050-ae34-c048001574b4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_ulong(139954445358592), c_ulong(139954445358080), c_ulong(139954445357568)]\n"
     ]
    }
   ],
   "source": [
    "result = m.calculate(dask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \n",
      "0  3.5\n",
      "1  4.5\n"
     ]
    }
   ],
   "source": [
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
