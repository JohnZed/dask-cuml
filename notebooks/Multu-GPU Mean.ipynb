{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Mean Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "60770839-8745-4b6d-8967-015bade0ce45"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "import cudf, dask_cudf\n",
    "from dask_cuml import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple ways to get data into cuml, which will need to be tested:\n",
    "1. A large cudf object could be created and then passed to dask_cudf\n",
    "2. The workers are asked to fetch the data directly\n",
    "\n",
    "Since this will likely be running in a single worker per GPU mode, it will be important that the cuDF's are able to work across the GPUs (e.g. When a very large cuDF is partitioned across the workers- it will be important that the GPU memory is re-allocated to the new worker's local device and de-allocated on the cuDF's old device.)\n",
    "\n",
    "__Example workflow__:\n",
    "- User allocates a dask_cudf (or, eventually, a dask_cuml_array) and distributes it across the cluster\n",
    "- User calls MGMean().calculate(dask_cudf) after the dask_cudf\n",
    "- MGMean performs redistribution / preprocessing\n",
    "- MGMean gathers allocations (hostname/device/key triplets) from Dask workers\n",
    "- MGMean c++ code is executed with the allocation information as its argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "ef9fe3a0-56dd-4152-9d7a-17230e76f9f2"
    }
   },
   "outputs": [],
   "source": [
    "client = Client(\"10.31.241.47:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "5d005f81-ffde-4df1-a9ab-61181f2c6748"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.31.241.47:8786\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>54.10 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.31.241.47:8786' processes=8 cores=8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cudf(dev):\n",
    "    import numba.cuda\n",
    "    import numpy as np\n",
    "    numba.cuda.select_device(dev)\n",
    "    logging.debug(\"Creating dataframe on device \" + str(dev))\n",
    "    return (dev, cudf.DataFrame(\n",
    "        [('a', np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]).astype(np.float32)), \n",
    "         ('b', np.array([2.0, 3.0, 4.0, 5.0, 6.0, 7.0]).astype(np.float32))]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcp://10.31.241.47:39120',\n",
       " 'tcp://10.31.241.47:34692',\n",
       " 'tcp://10.31.241.47:41735',\n",
       " 'tcp://10.31.241.47:36521',\n",
       " 'tcp://10.31.241.47:32818',\n",
       " 'tcp://10.31.241.47:43366',\n",
       " 'tcp://10.31.241.47:39638',\n",
       " 'tcp://10.31.241.47:45428']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers = list(client.has_what().keys())\n",
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2018, NVIDIA CORPORATION.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "from cuml import MGMean as cumlMGMean\n",
    "\n",
    "from tornado import gen\n",
    "import dask_cudf, cudf\n",
    "\n",
    "import logging\n",
    "\n",
    "import time\n",
    "\n",
    "from dask.distributed import get_worker, get_client\n",
    "\n",
    "from dask import delayed\n",
    "from collections import defaultdict\n",
    "from dask.distributed import wait, default_client\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "from toolz import first, assoc\n",
    "from distributed import Client\n",
    "\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "def to_gpu_matrix(inp):\n",
    "    dev, df = inp\n",
    "    import numba.cuda\n",
    "    numba.cuda.select_device(dev)\n",
    "    check_device(dev)\n",
    "    rm = df.as_gpu_matrix(order='F')\n",
    "    logging.debug(\"GPU: \" + str(rm))\n",
    "    logging.debug(\"CTYPES: \"+ str(rm.device_ctypes_pointer))\n",
    "    series = build_output_series(rm, dev)\n",
    "    return (dev, rm, series, to_gpu_array(series))\n",
    "\n",
    "def build_output_series(gpu_matrix, dev):\n",
    "    import numpy as np\n",
    "    import numba.cuda\n",
    "    numba.cuda.select_device(dev)\n",
    "    check_device(dev)\n",
    "    return cudf.Series(np.zeros(gpu_matrix.shape[1], dtype=gpu_matrix.dtype))\n",
    "\n",
    "def to_gpu_array(mean_):\n",
    "    return mean_._column._data.to_gpu_array()\n",
    "\n",
    "from threading import Thread\n",
    "import numba.cuda\n",
    "import time\n",
    "\n",
    "\n",
    "def check_device(dev):\n",
    "    if dev != numba.cuda.get_current_device().id:\n",
    "        logging.warn(\"Current device \" + \n",
    "                      str(numba.cuda.get_current_device()) + \n",
    "                      \" does not match expected \" + str(dev) +\n",
    "                      \". This could result in lowered performance.\")\n",
    "\n",
    "\n",
    "from threading import Lock\n",
    "\n",
    "class IPCThread(Thread):\n",
    "    \"\"\"\n",
    "    This mechanism gets around Numba's restriction of CUDA contexts being thread-local \n",
    "    by creating a thread that can select its own device. This allows the user of IPC \n",
    "    handles to open them up directly on the same device as the owner (bypassing the \n",
    "    need for peer access.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ipcs, device):\n",
    "        \n",
    "        Thread.__init__(self)\n",
    "\n",
    "        self.lock = Lock()\n",
    "        self.ipcs = ipcs\n",
    "        self.device = device\n",
    "        self.running = False\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        numba.cuda.select_device(self.device)\n",
    "        logging.debug(\"Opening: \" + str(self.device) + str(numba.cuda.get_current_device()))\n",
    "        \n",
    "        check_device(self.device)\n",
    "\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            self.arrs = [ipc.open() for ipc in self.ipcs]\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error opening ipc_handle on device \" + str(self.device) + \": \" + str(e))\n",
    "        \n",
    "        self.ptr_info = [x.__cuda_array_interface__ for x in self.arrs]\n",
    "        \n",
    "        self.running = True\n",
    "        self.lock.release()\n",
    "\n",
    "        while(self.running):\n",
    "            time.sleep(0.0001)\n",
    "        try:\n",
    "            logging.debug(\"Closing: \" + str(self.device) + str(numba.cuda.get_current_device()))\n",
    "            self.lock.acquire()\n",
    "            [ipc.close() for ipc in self.ipcs]\n",
    "            self.lock.release()\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error closing ipc_handle on device \" + str(self.device) + \": \" + str(e))\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        This should be called before calling join(). Otherwise, IPC handles may not be \n",
    "        properly cleaned up. \n",
    "        \"\"\"\n",
    "        self.lock.acquire()\n",
    "        self.running = False\n",
    "        self.lock.release()\n",
    "        \n",
    "    def info(self):\n",
    "        \"\"\"\n",
    "        Warning: this method is invoked from the calling thread. Make\n",
    "        sure the context in the thread reading the memory is tied to\n",
    "        self.device, otherwise an expensive peer access might take\n",
    "        place underneath.\n",
    "        \"\"\"\n",
    "        while(not self.running):\n",
    "            time.sleep(0.0001)\n",
    "            \n",
    "        return self.ptr_info\n",
    "\n",
    "\n",
    "def build_alloc_info(data):\n",
    "    dev, gpu_matrix, series, gpu_array = data\n",
    "    return [gpu_matrix.__cuda_array_interface__, gpu_array.__cuda_array_interface__]\n",
    "\n",
    "def get_ipc_handles(data):\n",
    "    \n",
    "    import numba.cuda\n",
    "    dev, gpu_matrix, series, gpu_array = data\n",
    "    check_device(dev)\n",
    "    \n",
    "    try:\n",
    "        in_handle = gpu_matrix.get_ipc_handle()\n",
    "        out_handle = gpu_matrix.get_ipc_handle()\n",
    "        return (dev, gpu_matrix.get_ipc_handle(), gpu_array.get_ipc_handle())\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error: \" + str(e))\n",
    "        return (dev, None, None)\n",
    "\n",
    "\n",
    "# Run on a single worker on each unique host\n",
    "def calc_mean(data):\n",
    "\n",
    "    import numba.cuda\n",
    "    print(\"begin calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    \n",
    "    ipcs, raw_arrs = data\n",
    "\n",
    "    def new_ipc_thread(dev, ipcs):\n",
    "        t = IPCThread(ipcs, dev)\n",
    "        t.start()\n",
    "        return t\n",
    "    \n",
    "    open_ipcs = [new_ipc_thread(dev, [inp, outp]) for dev, inp, outp in ipcs]\n",
    "    logging.debug(\"calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    m = cumlMGMean()\n",
    "    \n",
    "    alloc_info = [t.info() for t in open_ipcs]\n",
    "    alloc_info.extend([build_alloc_info(t) for t in raw_arrs])\n",
    "    \n",
    "    logging.debug(\"calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    m.calculate(alloc_info)\n",
    "\n",
    "    logging.debug(\"end calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    return open_ipcs, raw_arrs\n",
    "    \n",
    "class MGMean(object):\n",
    "\n",
    "    def calculate(self, futures):\n",
    "        client = default_client()\n",
    "\n",
    "        # Keep the futures around so the GPU memory doesn't get\n",
    "        # deallocated on the workers.\n",
    "        gpu_futures = client.sync(self._get_mg_info, futures)\n",
    "\n",
    "        who_has = client.who_has(gpu_futures)\n",
    "\n",
    "        key_to_host_dict = {}\n",
    "        for key in who_has:\n",
    "            key_to_host_dict[key] = parse_host_port(who_has[key][0])\n",
    "            \n",
    "        hosts_to_key_dict = {}\n",
    "        for key, host in key_to_host_dict.items():\n",
    "            if host not in hosts_to_key_dict:\n",
    "                hosts_to_key_dict[host] = set([key])\n",
    "            else:\n",
    "                hosts_to_key_dict[host].add(key)\n",
    "\n",
    "        workers = [key[0] for key in list(who_has.values())]\n",
    "        hosts_dict = build_host_dict(workers)\n",
    "        f = []\n",
    "        for host, ports in hosts_dict.items():\n",
    "            exec_node = (host, random.sample(ports, 1)[0])\n",
    "            \n",
    "            logging.debug(\"Chosen exec node is \"  + str(exec_node))\n",
    "            \n",
    "            # Don't build an ipc_handle for exec nodes (we can just grab the local data)\n",
    "            keys = set(hosts_to_key_dict[exec_node])\n",
    "            \n",
    "            # build ipc handles\n",
    "            gpu_data_excl_worker = filter(lambda d: d[0] != exec_node, gpu_futures)\n",
    "            gpu_data_incl_worker = filter(lambda d: d[0] == exec_node, gpu_futures)\n",
    "            \n",
    "            ipc_handles = [client.submit(get_ipc_handles, future, workers=[worker])\n",
    "                           for worker, future in gpu_data_excl_worker]\n",
    "            raw_arrays = [future for worker, future in gpu_data_incl_worker]\n",
    "            \n",
    "            logging.debug(str(ipc_handles))\n",
    "            logging.debug(str(raw_arrays))\n",
    "            \n",
    "            f.append(client.submit(calc_mean, (ipc_handles, raw_arrays), workers = [exec_node]))\n",
    "\n",
    "        wait(f)\n",
    "        \n",
    "        def close_threads(d):\n",
    "            logging.debug(str(\"Closing threads!\"))\n",
    "            ipc_threads, rawarrays = d\n",
    "            [t.close() for t in ipc_threads]\n",
    "            \n",
    "        d = [client.submit(close_threads, future) for future in f]\n",
    "        wait(d)\n",
    "        \n",
    "        def join_threads(d):\n",
    "            logging.debug(str(\"Joining threads!\"))\n",
    "            ipc_threads, rawarrays = d\n",
    "            [t.join() for t in ipc_threads]\n",
    "            \n",
    "        d = [client.submit(join_threads, future) for future in f]\n",
    "        \n",
    "        def print_it(data):\n",
    "            dev, gpu_mat, series, gpu_arr = data\n",
    "            return str(series)\n",
    "\n",
    "        return client.gather([client.submit(print_it, future) for worker, future in gpu_futures])\n",
    "    \n",
    "\n",
    "    @gen.coroutine\n",
    "    def _get_mg_info(self, futures):\n",
    "\n",
    "        client = default_client()\n",
    "\n",
    "        if isinstance(futures, dd.DataFrame):\n",
    "            data_parts = futures.to_delayed()\n",
    "            parts = list(map(delayed, data_parts))\n",
    "            parts = client.compute(parts)  # Start computation in the background\n",
    "            yield wait(parts)\n",
    "            for part in parts:\n",
    "                if part.status == 'error':\n",
    "                    yield part  # trigger error locally\n",
    "        else:\n",
    "            data_parts = futures\n",
    "\n",
    "        key_to_part_dict = dict([(str(part.key), part) for part in data_parts])\n",
    "\n",
    "        who_has = yield client.who_has(data_parts)\n",
    "        worker_map = []\n",
    "        for key, workers in who_has.items():\n",
    "            \n",
    "            worker = parse_host_port(first(workers))\n",
    "            worker_map.append((worker, key_to_part_dict[key]))\n",
    "\n",
    "        gpu_data = [(worker, client.submit(to_gpu_matrix, part, workers=[worker]))\n",
    "                    for worker, part in worker_map]\n",
    "        \n",
    "        yield wait(gpu_data)\n",
    "\n",
    "        raise gen.Return(gpu_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "import random\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "def build_host_dict(workers):\n",
    "    hosts = set(map(lambda x: parse_host_port(x), workers))\n",
    "    hosts_dict = {}\n",
    "    for host, port in hosts:\n",
    "        if host not in hosts_dict:\n",
    "            hosts_dict[host] = set([port])\n",
    "        else:\n",
    "            hosts_dict[host].add(port)\n",
    "            \n",
    "    return hosts_dict\n",
    "    \n",
    "\n",
    "def assign_gpus(client):\n",
    "    \n",
    "    \"\"\"\n",
    "    Supports a multi-GPU & multi-Node environment by assigning a single local GPU\n",
    "    to each worker in the cluster. This is necessary due to Numba's restriction\n",
    "    that only a single CUDA context (and thus a single device) can be active on a \n",
    "    thread at a time. \n",
    "    \n",
    "    The GPU assignments are valid as long as the future returned from this function\n",
    "    is held in scope. This allows any functions that need to allocate GPU data to\n",
    "    utilize the CUDA context on the same device, otherwise data could be lost.\n",
    "    \"\"\"\n",
    "\n",
    "    workers = list(client.has_what().keys())\n",
    "    hosts_dict = build_host_dict(workers)\n",
    "    \n",
    "    def get_gpu_info():\n",
    "        import numba.cuda\n",
    "        return [x.id for x in numba.cuda.gpus]\n",
    "    \n",
    "    gpu_info = dict([(host, \n",
    "                      client.submit(get_gpu_info, \n",
    "                                    workers = [(host, random.sample(hosts_dict[host], 1)[0])])) \n",
    "                     for host in hosts_dict])\n",
    "    wait(list(gpu_info.values()))\n",
    "    \n",
    "    # Scatter out a GPU device ID to workers\n",
    "    f = []\n",
    "    for host, future in gpu_info.items():\n",
    "        gpu_ids = future.result()\n",
    "        ports = random.sample(hosts_dict[host], min(len(gpu_ids), len(hosts_dict[host])))\n",
    "        \n",
    "        f.extend([client.scatter(device_id, workers = [(host,port)]) for device_id, port in zip(gpu_ids, ports)])\n",
    "    wait(f)\n",
    "        \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = assign_gpus(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'create_cudf-1061f27a156e69abc48b9d8f28ab094c': ('tcp://10.31.241.47:39638',),\n",
       " 'create_cudf-12b4ebdab71e5fed1115fb6601d9bd3b': ('tcp://10.31.241.47:43366',),\n",
       " 'create_cudf-32018a29c137d10bc927cfc09f688d6a': ('tcp://10.31.241.47:34692',),\n",
       " 'create_cudf-529975090709e867d21a78db55045ea2': ('tcp://10.31.241.47:32818',),\n",
       " 'create_cudf-92e5a84baff59ec4abc4fe01cd531ad4': ('tcp://10.31.241.47:45428',),\n",
       " 'create_cudf-af457634ff7347c0060d1cf27548c9a9': ('tcp://10.31.241.47:41735',),\n",
       " 'create_cudf-b63b42d1f3ea3d90a7cf277f9ba12bb3': ('tcp://10.31.241.47:39120',),\n",
       " 'create_cudf-be4ade7a2625c915b15a25eba948b141': ('tcp://10.31.241.47:36521',),\n",
       " 'int-06e5a71c9839bd98760be56f629b24cc': ('tcp://10.31.241.47:45428',\n",
       "  'tcp://10.31.241.47:39120',\n",
       "  'tcp://10.31.241.47:43366'),\n",
       " 'int-58e78e1b34eb49a68c65b54815d1b158': ('tcp://10.31.241.47:39638',\n",
       "  'tcp://10.31.241.47:36521',\n",
       "  'tcp://10.31.241.47:41735'),\n",
       " 'int-5c8a950061aa331153f4a172bbcbfd1b': ('tcp://10.31.241.47:39638',\n",
       "  'tcp://10.31.241.47:39120',\n",
       "  'tcp://10.31.241.47:32818',\n",
       "  'tcp://10.31.241.47:34692'),\n",
       " 'int-5cd9541ea58b401f115b751e79eabbff': ('tcp://10.31.241.47:34692',\n",
       "  'tcp://10.31.241.47:36521',\n",
       "  'tcp://10.31.241.47:41735',\n",
       "  'tcp://10.31.241.47:32818'),\n",
       " 'int-7ec5d3339274cee5cb507a4e4d28e791': ('tcp://10.31.241.47:43366',\n",
       "  'tcp://10.31.241.47:36521',\n",
       "  'tcp://10.31.241.47:39638',\n",
       "  'tcp://10.31.241.47:41735'),\n",
       " 'int-c0a8a20f903a4915b94db8de3ea63195': ('tcp://10.31.241.47:34692',\n",
       "  'tcp://10.31.241.47:39638',\n",
       "  'tcp://10.31.241.47:39120',\n",
       "  'tcp://10.31.241.47:36521',\n",
       "  'tcp://10.31.241.47:45428'),\n",
       " 'int-ce9a05dd6ec76c6a6d171b0c055f3127': ('tcp://10.31.241.47:45428',\n",
       "  'tcp://10.31.241.47:39120',\n",
       "  'tcp://10.31.241.47:32818',\n",
       "  'tcp://10.31.241.47:43366'),\n",
       " 'int-d3395e15f605bc35ab1bac6341a285e2': ('tcp://10.31.241.47:45428',\n",
       "  'tcp://10.31.241.47:36521',\n",
       "  'tcp://10.31.241.47:39638',\n",
       "  'tcp://10.31.241.47:34692')}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done={<Future: status: finished, type: tuple, key: create_cudf-529975090709e867d21a78db55045ea2>, <Future: status: finished, type: tuple, key: create_cudf-1061f27a156e69abc48b9d8f28ab094c>, <Future: status: finished, type: tuple, key: create_cudf-af457634ff7347c0060d1cf27548c9a9>, <Future: status: finished, type: tuple, key: create_cudf-12b4ebdab71e5fed1115fb6601d9bd3b>, <Future: status: finished, type: tuple, key: create_cudf-92e5a84baff59ec4abc4fe01cd531ad4>, <Future: status: finished, type: tuple, key: create_cudf-32018a29c137d10bc927cfc09f688d6a>, <Future: status: finished, type: tuple, key: create_cudf-be4ade7a2625c915b15a25eba948b141>, <Future: status: finished, type: tuple, key: create_cudf-b63b42d1f3ea3d90a7cf277f9ba12bb3>}, not_done=set())"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [client.submit(create_cudf, future, workers = [worker]) for future, worker in zip(assignments, workers)]\n",
    "wait(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'create_cudf-1061f27a156e69abc48b9d8f28ab094c': ('tcp://10.31.241.47:39638',),\n",
       " 'create_cudf-12b4ebdab71e5fed1115fb6601d9bd3b': ('tcp://10.31.241.47:43366',),\n",
       " 'create_cudf-32018a29c137d10bc927cfc09f688d6a': ('tcp://10.31.241.47:34692',),\n",
       " 'create_cudf-529975090709e867d21a78db55045ea2': ('tcp://10.31.241.47:32818',),\n",
       " 'create_cudf-92e5a84baff59ec4abc4fe01cd531ad4': ('tcp://10.31.241.47:45428',),\n",
       " 'create_cudf-af457634ff7347c0060d1cf27548c9a9': ('tcp://10.31.241.47:41735',),\n",
       " 'create_cudf-b63b42d1f3ea3d90a7cf277f9ba12bb3': ('tcp://10.31.241.47:39120',),\n",
       " 'create_cudf-be4ade7a2625c915b15a25eba948b141': ('tcp://10.31.241.47:36521',),\n",
       " 'int-06e5a71c9839bd98760be56f629b24cc': ('tcp://10.31.241.47:45428',\n",
       "  'tcp://10.31.241.47:39120',\n",
       "  'tcp://10.31.241.47:43366'),\n",
       " 'int-58e78e1b34eb49a68c65b54815d1b158': ('tcp://10.31.241.47:39638',\n",
       "  'tcp://10.31.241.47:36521',\n",
       "  'tcp://10.31.241.47:41735'),\n",
       " 'int-5c8a950061aa331153f4a172bbcbfd1b': ('tcp://10.31.241.47:39638',\n",
       "  'tcp://10.31.241.47:39120',\n",
       "  'tcp://10.31.241.47:32818',\n",
       "  'tcp://10.31.241.47:34692'),\n",
       " 'int-5cd9541ea58b401f115b751e79eabbff': ('tcp://10.31.241.47:34692',\n",
       "  'tcp://10.31.241.47:36521',\n",
       "  'tcp://10.31.241.47:41735',\n",
       "  'tcp://10.31.241.47:32818'),\n",
       " 'int-7ec5d3339274cee5cb507a4e4d28e791': ('tcp://10.31.241.47:43366',\n",
       "  'tcp://10.31.241.47:36521',\n",
       "  'tcp://10.31.241.47:39638',\n",
       "  'tcp://10.31.241.47:41735'),\n",
       " 'int-c0a8a20f903a4915b94db8de3ea63195': ('tcp://10.31.241.47:34692',\n",
       "  'tcp://10.31.241.47:39638',\n",
       "  'tcp://10.31.241.47:39120',\n",
       "  'tcp://10.31.241.47:36521',\n",
       "  'tcp://10.31.241.47:45428'),\n",
       " 'int-ce9a05dd6ec76c6a6d171b0c055f3127': ('tcp://10.31.241.47:45428',\n",
       "  'tcp://10.31.241.47:39120',\n",
       "  'tcp://10.31.241.47:32818',\n",
       "  'tcp://10.31.241.47:43366'),\n",
       " 'int-d3395e15f605bc35ab1bac6341a285e2': ('tcp://10.31.241.47:45428',\n",
       "  'tcp://10.31.241.47:36521',\n",
       "  'tcp://10.31.241.47:39638',\n",
       "  'tcp://10.31.241.47:34692')}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "nbpresent": {
     "id": "058fd901-c613-4d64-88cb-fccc3181a269"
    }
   },
   "outputs": [],
   "source": [
    "m = MGMean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:Chosen exec node is ('10.31.241.47', 32818)\n",
      "DEBUG:[<Future: status: pending, key: get_ipc_handles-2f54e0237a42be03bb830d81ab3bed47>, <Future: status: pending, key: get_ipc_handles-3c5f2e6cfd750509650928c3342dedb0>, <Future: status: pending, key: get_ipc_handles-86be7d40d5944f014e1dedcbbdf38b73>, <Future: status: pending, key: get_ipc_handles-6395a9d5c900b02427322665b8c2ae51>, <Future: status: pending, key: get_ipc_handles-8987247991c6885caec835d903c82053>, <Future: status: pending, key: get_ipc_handles-48cc9e14dcb666d1f084708291a1410a>, <Future: status: pending, key: get_ipc_handles-4dbc566ab0124746f0707240d2c58f3c>]\n",
      "DEBUG:[<Future: status: finished, type: tuple, key: to_gpu_matrix-bfa84d1b44f20bfa00c5fb1dc2f35882>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.calculate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-b00e420a6399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.who_has()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
