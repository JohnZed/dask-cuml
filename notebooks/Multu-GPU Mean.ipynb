{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Mean Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "nbpresent": {
     "id": "60770839-8745-4b6d-8967-015bade0ce45"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "import cudf, dask_cudf\n",
    "from dask_cuml import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple ways to get data into cuml, which will need to be tested:\n",
    "1. A large cudf object could be created and then passed to dask_cudf\n",
    "2. The workers are asked to fetch the data directly\n",
    "\n",
    "Since this will likely be running in a single worker per GPU mode, it will be important that the cuDF's are able to work across the GPUs (e.g. When a very large cuDF is partitioned across the workers- it will be important that the GPU memory is re-allocated to the new worker's local device and de-allocated on the cuDF's old device.)\n",
    "\n",
    "__Example workflow__:\n",
    "- User allocates a dask_cudf (or, eventually, a dask_cuml_array) and distributes it across the cluster\n",
    "- User calls MGMean().calculate(dask_cudf) after the dask_cudf\n",
    "- MGMean performs redistribution / preprocessing\n",
    "- MGMean gathers allocations (hostname/device/key triplets) from Dask workers\n",
    "- MGMean c++ code is executed with the allocation information as its argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "nbpresent": {
     "id": "ef9fe3a0-56dd-4152-9d7a-17230e76f9f2"
    }
   },
   "outputs": [],
   "source": [
    "client = Client(\"10.31.241.47:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "nbpresent": {
     "id": "5d005f81-ffde-4df1-a9ab-61181f2c6748"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.31.241.47:8786\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>5</li>\n",
       "  <li><b>Memory: </b>33.81 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.31.241.47:8786' processes=5 cores=5>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cudf(dev):\n",
    "    import numba.cuda\n",
    "    import numpy as np\n",
    "    numba.cuda.select_device(dev)\n",
    "    print(\"Creating dataframe on device \" + str(dev))\n",
    "    return (dev, cudf.DataFrame(\n",
    "        [('a', np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]).astype(np.float32)), \n",
    "         ('b', np.array([2.0, 3.0, 4.0, 5.0, 6.0, 7.0]).astype(np.float32))]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcp://10.31.241.47:33516',\n",
       " 'tcp://10.31.241.47:41165',\n",
       " 'tcp://10.31.241.47:35201',\n",
       " 'tcp://10.31.241.47:46597',\n",
       " 'tcp://10.31.241.47:38489']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers = list(client.has_what().keys())\n",
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2018, NVIDIA CORPORATION.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "from cuml import MGMean as cumlMGMean\n",
    "\n",
    "from tornado import gen\n",
    "import dask_cudf, cudf\n",
    "\n",
    "import time\n",
    "\n",
    "from dask.distributed import get_worker, get_client\n",
    "\n",
    "from dask import delayed\n",
    "from collections import defaultdict\n",
    "from dask.distributed import wait, default_client\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "from toolz import first, assoc\n",
    "from distributed import Client\n",
    "\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "def to_gpu_matrix(inp):\n",
    "    dev, df = inp\n",
    "    import numba.cuda\n",
    "    numba.cuda.select_device(dev)\n",
    "    print(str(numba.cuda.get_current_device()) + \" - \" + str(dev))\n",
    "    rm = df.as_gpu_matrix(order='F')\n",
    "    print(\"GPU: \" + str(rm))\n",
    "    print(\"CTYPES: \"+ str(rm.device_ctypes_pointer))\n",
    "    series = build_output_series(rm, dev)\n",
    "    return (dev, rm, series, to_gpu_array(series))\n",
    "\n",
    "def build_output_series(gpu_matrix, dev):\n",
    "    import numpy as np\n",
    "    import numba.cuda\n",
    "    numba.cuda.select_device(dev)\n",
    "    print(str(numba.cuda.get_current_device()) + \" - \" + str(dev))\n",
    "    return cudf.Series(np.zeros(gpu_matrix.shape[1], dtype=gpu_matrix.dtype))\n",
    "\n",
    "def to_gpu_array(mean_):\n",
    "    return mean_._column._data.to_gpu_array()\n",
    "\n",
    "from threading import Thread\n",
    "import numba.cuda\n",
    "import time\n",
    "\n",
    "from threading import Lock\n",
    "\n",
    "class IPCThread(Thread):\n",
    "    \n",
    "    def __init__(self, in_ipc, out_ipc, dev):\n",
    "        Thread.__init__(self)\n",
    "\n",
    "        self.lock = Lock()\n",
    "        self.in_ipc = in_ipc\n",
    "        self.out_ipc = out_ipc\n",
    "        self.dev = dev\n",
    "        self.running = False\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        numba.cuda.select_device(self.dev)\n",
    "        print(\"Opening: \" + str(self.dev) + str(numba.cuda.get_current_device()))\n",
    "        \n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            self.in_arr = self.in_ipc.open()\n",
    "            self.out_arr = self.out_ipc.open()\n",
    "        except Exception as e:\n",
    "            print(\"Error opening ipc_handle on device \" + str(self.dev) + \": \" + str(e))\n",
    "        \n",
    "        cai = self.in_arr.__cuda_array_interface__\n",
    "        self.ptr_info = {\"ptr\": self.in_arr.device_ctypes_pointer.value,\n",
    "                \"out_ptr\": self.out_arr.device_ctypes_pointer.value,\n",
    "                \"dtype\": cai[\"typestr\"],\n",
    "                \"shape\": cai[\"shape\"]\n",
    "        }\n",
    "        \n",
    "        self.running = True\n",
    "        self.lock.release()\n",
    "\n",
    "        while(self.running):\n",
    "            time.sleep(0.0001)\n",
    "        try:\n",
    "            print(\"Closing: \" + str(self.dev) + str(numba.cuda.get_current_device()))\n",
    "\n",
    "            self.lock.acquire()\n",
    "            self.in_ipc.close()\n",
    "            self.out_ipc.close()\n",
    "            self.lock.release()\n",
    "        except Exception as e:\n",
    "            print(\"Error closing ipc_handle on device \" + str(self.dev) + \": \" + str(e))\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        \n",
    "        self.lock.acquire()\n",
    "        self.running = False\n",
    "        self.lock.release()\n",
    "        \n",
    "    def info(self):\n",
    "        \n",
    "        while(not self.running):\n",
    "            time.sleep(0.0001)\n",
    "            \n",
    "        return self.ptr_info\n",
    "\n",
    "    \n",
    "def build_alloc_info(data):\n",
    "    dev, gpu_matrix, series, gpu_array = data\n",
    "    \n",
    "    cai = gpu_matrix.__cuda_array_interface__\n",
    "    return {\"ptr\": gpu_matrix.device_ctypes_pointer.value,\n",
    "        \"out_ptr\": gpu_array.device_ctypes_pointer.value,\n",
    "        \"dtype\": cai[\"typestr\"],\n",
    "        \"shape\": cai[\"shape\"]\n",
    "    }\n",
    "    \n",
    "\n",
    "def get_ipc_handles(data):\n",
    "    \n",
    "    import numba.cuda\n",
    "    print(\"CURRENT DEVICE: \" + str(numba.cuda.get_current_device()))\n",
    "    dev, gpu_matrix, series, gpu_array = data\n",
    "    \n",
    "    try:\n",
    "        in_handle = gpu_matrix.get_ipc_handle()\n",
    "        out_handle = gpu_matrix.get_ipc_handle()\n",
    "        \n",
    "        return (dev, gpu_matrix.get_ipc_handle(), gpu_array.get_ipc_handle())\n",
    "    except Exception as e:\n",
    "        print(\"Error: \" + str(e))\n",
    "        return (dev, None, None)\n",
    "\n",
    "\n",
    "# Run on a single worker on each unique host\n",
    "def calc_mean(data):\n",
    "\n",
    "    import numba.cuda\n",
    "    print(\"calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    \n",
    "    ipcs, raw_arrs = data\n",
    "\n",
    "    def get_ipc_thread(dev, inp, outp):\n",
    "        t = IPCThread(inp, outp, dev)\n",
    "        t.start()\n",
    "        return t\n",
    "\n",
    "    open_ipcs = [get_ipc_thread(dev, inp, outp) for dev, inp, outp in ipcs]\n",
    "    print(\"calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    m = cumlMGMean()\n",
    "    \n",
    "    alloc_info = [t.info() for t in open_ipcs]\n",
    "    alloc_info.extend([build_alloc_info(t) for t in raw_arrs])\n",
    "    \n",
    "    print(\"calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    m.calculate(alloc_info)\n",
    "\n",
    "    print(\"calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    return open_ipcs, raw_arrs\n",
    "    \n",
    "class MGMean(object):\n",
    "\n",
    "    def calculate(self, futures):\n",
    "        client = default_client()\n",
    "\n",
    "        # Keep the futures around so the GPU memory doesn't get\n",
    "        # deallocated on the workers.\n",
    "        gpu_futures = client.sync(self._get_mg_info, futures)\n",
    "\n",
    "        who_has = client.who_has(gpu_futures)\n",
    "\n",
    "        key_to_host_dict = {}\n",
    "        for key in who_has:\n",
    "            key_to_host_dict[key] = parse_host_port(who_has[key][0])\n",
    "            \n",
    "        hosts_to_key_dict = {}\n",
    "        for key, host in key_to_host_dict.items():\n",
    "            if host not in hosts_to_key_dict:\n",
    "                hosts_to_key_dict[host] = set([key])\n",
    "            else:\n",
    "                hosts_to_key_dict[host].add(key)\n",
    "\n",
    "        workers = [key[0] for key in list(who_has.values())]\n",
    "        hosts_dict = build_host_dict(workers)\n",
    "        f = []\n",
    "        for host, ports in hosts_dict.items():\n",
    "            exec_node = (host, random.sample(ports, 1)[0])\n",
    "            \n",
    "            print(\"Chosen exec node is \"  + str(exec_node))\n",
    "            \n",
    "            # Don't build an ipc_handle for exec nodes (we can just grab the local data)\n",
    "            keys = set(hosts_to_key_dict[exec_node])\n",
    "            \n",
    "            # build ipc handles\n",
    "            gpu_data_excl_worker = filter(lambda d: d[0] != exec_node, gpu_futures)\n",
    "            gpu_data_incl_worker = filter(lambda d: d[0] == exec_node, gpu_futures)\n",
    "            \n",
    "            ipc_handles = [client.submit(get_ipc_handles, future, workers=[worker])\n",
    "                           for worker, future in gpu_data_excl_worker]\n",
    "            raw_arrays = [future for worker, future in gpu_data_incl_worker]\n",
    "            \n",
    "            print(str(ipc_handles))\n",
    "            print(str(raw_arrays))\n",
    "            \n",
    "            f.append(client.submit(calc_mean, (ipc_handles, raw_arrays), workers = [exec_node]))\n",
    "\n",
    "        wait(f)\n",
    "        \n",
    "        def close_threads(d):\n",
    "            print(str(\"Closing threads!\"))\n",
    "            ipc_threads, rawarrays = d\n",
    "            [t.close() for t in ipc_threads]\n",
    "            \n",
    "        d = [client.submit(close_threads, future) for future in f]\n",
    "        wait(d)\n",
    "        \n",
    "        def join_threads(d):\n",
    "            print(str(\"Joining threads!\"))\n",
    "            ipc_threads, rawarrays = d\n",
    "            [t.join() for t in ipc_threads]\n",
    "            \n",
    "        d = [client.submit(join_threads, future) for future in f]\n",
    "        \n",
    "        def print_it(data):\n",
    "            dev, gpu_mat, series, gpu_arr = data\n",
    "            return str(series)\n",
    "\n",
    "        return client.gather([client.submit(print_it, future) for worker, future in gpu_futures])\n",
    "    \n",
    "\n",
    "    @gen.coroutine\n",
    "    def _get_mg_info(self, futures):\n",
    "\n",
    "        client = default_client()\n",
    "\n",
    "        if isinstance(futures, dd.DataFrame):\n",
    "            data_parts = futures.to_delayed()\n",
    "            parts = list(map(delayed, data_parts))\n",
    "            parts = client.compute(parts)  # Start computation in the background\n",
    "            yield wait(parts)\n",
    "            for part in parts:\n",
    "                if part.status == 'error':\n",
    "                    yield part  # trigger error locally\n",
    "        else:\n",
    "            data_parts = futures\n",
    "\n",
    "        key_to_part_dict = dict([(str(part.key), part) for part in data_parts])\n",
    "\n",
    "        who_has = yield client.who_has(data_parts)\n",
    "        worker_map = []\n",
    "        for key, workers in who_has.items():\n",
    "            \n",
    "            worker = parse_host_port(first(workers))\n",
    "            worker_map.append((worker, key_to_part_dict[key]))\n",
    "\n",
    "        gpu_data = [(worker, client.submit(to_gpu_matrix, part, workers=[worker]))\n",
    "                    for worker, part in worker_map]\n",
    "        \n",
    "        yield wait(gpu_data)\n",
    "\n",
    "        raise gen.Return(gpu_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "import random\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "def build_host_dict(workers):\n",
    "    hosts = set(map(lambda x: parse_host_port(x), workers))\n",
    "    hosts_dict = {}\n",
    "    for host, port in hosts:\n",
    "        if host not in hosts_dict:\n",
    "            hosts_dict[host] = set([port])\n",
    "        else:\n",
    "            hosts_dict[host].add(port)\n",
    "            \n",
    "    return hosts_dict\n",
    "    \n",
    "\n",
    "def assign_gpus(client):\n",
    "    \n",
    "    \"\"\"\n",
    "    Supports a multi-GPU & multi-Node environment by assigning a single local GPU\n",
    "    to each worker in the cluster. This is necessary due to Numba's restriction\n",
    "    that only a single CUDA context (and thus a single device) can be active on a \n",
    "    thread at a time. \n",
    "    \n",
    "    The GPU assignments are valid as long as the future returned from this function\n",
    "    is held in scope. This allows any functions that need to allocate GPU data to\n",
    "    utilize the CUDA context on the same device, otherwise data could be lost.\n",
    "    \"\"\"\n",
    "\n",
    "    workers = list(client.has_what().keys())\n",
    "    hosts_dict = build_host_dict(workers)\n",
    "    \n",
    "    def get_gpu_info():\n",
    "        import numba.cuda\n",
    "        return [x.id for x in numba.cuda.gpus]\n",
    "    \n",
    "    gpu_info = dict([(host, \n",
    "                      client.submit(get_gpu_info, \n",
    "                                    workers = [(host, random.sample(hosts_dict[host], 1)[0])])) \n",
    "                     for host in hosts_dict])\n",
    "    wait(list(gpu_info.values()))\n",
    "    \n",
    "    # Scatter out a GPU device ID to workers\n",
    "    f = []\n",
    "    for host, future in gpu_info.items():\n",
    "        gpu_ids = future.result()\n",
    "        ports = random.sample(hosts_dict[host], min(len(gpu_ids), len(hosts_dict[host])))\n",
    "        \n",
    "        f.extend([client.scatter(device_id, workers = [(host,port)]) for device_id, port in zip(gpu_ids, ports)])\n",
    "    wait(f)\n",
    "        \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = assign_gpus(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calc_mean-d14aa6846e36c0b876d1e67ce6c69330': ('tcp://10.31.241.47:41165',),\n",
       " 'create_cudf-40ef018e7d18ab18a969bc2f9f3e32cd': ('tcp://10.31.241.47:41165',),\n",
       " 'create_cudf-5917e56cda471a44f4834546bfcac9ba': ('tcp://10.31.241.47:38489',),\n",
       " 'create_cudf-8d78d86a7729d3150eb1b8f7b0cc4c4d': ('tcp://10.31.241.47:46597',),\n",
       " 'create_cudf-96c79b3959983f80afb0e61ab1df27a1': ('tcp://10.31.241.47:35201',),\n",
       " 'create_cudf-e2fc6683845ae641dac2c0a379b81ca2': ('tcp://10.31.241.47:33516',),\n",
       " 'get_ipc_handles-28ff925961d748ead9ffcdda6781f9d9': ('tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'get_ipc_handles-3e3a0679c38607409c418eb75840639a': ('tcp://10.31.241.47:33516',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'get_ipc_handles-47071f2913142f39eb4c2d6526eb10a5': ('tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'get_ipc_handles-508189c060da94aebb9bb373d9fc86f7': ('tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'int-58e78e1b34eb49a68c65b54815d1b158': ('tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:33516'),\n",
       " 'int-5c8a950061aa331153f4a172bbcbfd1b': ('tcp://10.31.241.47:33516',\n",
       "  'tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:38489'),\n",
       " 'int-5cd9541ea58b401f115b751e79eabbff': ('tcp://10.31.241.47:33516',\n",
       "  'tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:38489'),\n",
       " 'int-c0a8a20f903a4915b94db8de3ea63195': ('tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:33516'),\n",
       " 'int-d3395e15f605bc35ab1bac6341a285e2': ('tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:33516'),\n",
       " 'join_threads-472b47d5bf108264df36737924f42dc7': ('tcp://10.31.241.47:41165',),\n",
       " 'print_it-ad2d7b657702c4c69ed553e7aed4ec4d': ('tcp://10.31.241.47:41165',),\n",
       " 'print_it-b7b5c11e4aab7376caffbc2c206410a0': ('tcp://10.31.241.47:33516',),\n",
       " 'print_it-cacdcf21d37104c8064bf6f672447736': ('tcp://10.31.241.47:46597',),\n",
       " 'print_it-d9fc044eb5952df6cad623d32726eb11': ('tcp://10.31.241.47:38489',),\n",
       " 'print_it-e8883d3a12e710d562f381deacd1f32f': ('tcp://10.31.241.47:35201',),\n",
       " 'to_gpu_matrix-07a0d737b8775c5f17a82da87b656b9e': ('tcp://10.31.241.47:46597',),\n",
       " 'to_gpu_matrix-3e66533af6962cb849705574d56a1fc6': ('tcp://10.31.241.47:35201',),\n",
       " 'to_gpu_matrix-74792332d08d267a8da00ab952de37d1': ('tcp://10.31.241.47:38489',),\n",
       " 'to_gpu_matrix-87e72d9fc96ad48003e4664b5a49714c': ('tcp://10.31.241.47:33516',),\n",
       " 'to_gpu_matrix-a815667676dd9ba17c7c2ce0bdbc2c3f': ('tcp://10.31.241.47:41165',)}"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done={<Future: status: finished, type: tuple, key: create_cudf-40ef018e7d18ab18a969bc2f9f3e32cd>, <Future: status: finished, type: tuple, key: create_cudf-e2fc6683845ae641dac2c0a379b81ca2>, <Future: status: finished, type: tuple, key: create_cudf-5917e56cda471a44f4834546bfcac9ba>, <Future: status: finished, type: tuple, key: create_cudf-8d78d86a7729d3150eb1b8f7b0cc4c4d>, <Future: status: finished, type: tuple, key: create_cudf-96c79b3959983f80afb0e61ab1df27a1>}, not_done=set())"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [client.submit(create_cudf, future, workers = [worker]) for future, worker in zip(assignments, workers)]\n",
    "wait(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calc_mean-d14aa6846e36c0b876d1e67ce6c69330': ('tcp://10.31.241.47:41165',),\n",
       " 'create_cudf-40ef018e7d18ab18a969bc2f9f3e32cd': ('tcp://10.31.241.47:41165',),\n",
       " 'create_cudf-5917e56cda471a44f4834546bfcac9ba': ('tcp://10.31.241.47:38489',),\n",
       " 'create_cudf-8d78d86a7729d3150eb1b8f7b0cc4c4d': ('tcp://10.31.241.47:46597',),\n",
       " 'create_cudf-96c79b3959983f80afb0e61ab1df27a1': ('tcp://10.31.241.47:35201',),\n",
       " 'create_cudf-e2fc6683845ae641dac2c0a379b81ca2': ('tcp://10.31.241.47:33516',),\n",
       " 'get_ipc_handles-28ff925961d748ead9ffcdda6781f9d9': ('tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'get_ipc_handles-3e3a0679c38607409c418eb75840639a': ('tcp://10.31.241.47:33516',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'get_ipc_handles-47071f2913142f39eb4c2d6526eb10a5': ('tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'get_ipc_handles-508189c060da94aebb9bb373d9fc86f7': ('tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'int-58e78e1b34eb49a68c65b54815d1b158': ('tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:33516'),\n",
       " 'int-5c8a950061aa331153f4a172bbcbfd1b': ('tcp://10.31.241.47:33516',\n",
       "  'tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:38489'),\n",
       " 'int-5cd9541ea58b401f115b751e79eabbff': ('tcp://10.31.241.47:33516',\n",
       "  'tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:38489'),\n",
       " 'int-c0a8a20f903a4915b94db8de3ea63195': ('tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:33516'),\n",
       " 'int-d3395e15f605bc35ab1bac6341a285e2': ('tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:33516'),\n",
       " 'join_threads-472b47d5bf108264df36737924f42dc7': ('tcp://10.31.241.47:41165',),\n",
       " 'print_it-ad2d7b657702c4c69ed553e7aed4ec4d': ('tcp://10.31.241.47:41165',),\n",
       " 'print_it-b7b5c11e4aab7376caffbc2c206410a0': ('tcp://10.31.241.47:33516',),\n",
       " 'print_it-cacdcf21d37104c8064bf6f672447736': ('tcp://10.31.241.47:46597',),\n",
       " 'print_it-d9fc044eb5952df6cad623d32726eb11': ('tcp://10.31.241.47:38489',),\n",
       " 'print_it-e8883d3a12e710d562f381deacd1f32f': ('tcp://10.31.241.47:35201',),\n",
       " 'to_gpu_matrix-07a0d737b8775c5f17a82da87b656b9e': ('tcp://10.31.241.47:46597',),\n",
       " 'to_gpu_matrix-3e66533af6962cb849705574d56a1fc6': ('tcp://10.31.241.47:35201',),\n",
       " 'to_gpu_matrix-74792332d08d267a8da00ab952de37d1': ('tcp://10.31.241.47:38489',),\n",
       " 'to_gpu_matrix-87e72d9fc96ad48003e4664b5a49714c': ('tcp://10.31.241.47:33516',),\n",
       " 'to_gpu_matrix-a815667676dd9ba17c7c2ce0bdbc2c3f': ('tcp://10.31.241.47:41165',)}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "nbpresent": {
     "id": "058fd901-c613-4d64-88cb-fccc3181a269"
    }
   },
   "outputs": [],
   "source": [
    "m = MGMean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen exec node is ('10.31.241.47', 38489)\n",
      "[<Future: status: pending, key: get_ipc_handles-f2934afc1fb9dc757d6ddf4b7d153c87>, <Future: status: pending, key: get_ipc_handles-6cc70fec43102542ddf7b7cb1555c422>, <Future: status: pending, key: get_ipc_handles-46c1e557a8bed5898d4803721d0e8e1d>, <Future: status: pending, key: get_ipc_handles-2597a091f21738f85c10689df966fccb>]\n",
      "[<Future: status: finished, type: tuple, key: to_gpu_matrix-8965f172b71e33b21bbaa01a9b8ce0d1>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5',\n",
       " '      \\n0  3.5\\n1  4.5']"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.calculate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calc_mean-d14aa6846e36c0b876d1e67ce6c69330': ('tcp://10.31.241.47:41165',),\n",
       " 'create_cudf-40ef018e7d18ab18a969bc2f9f3e32cd': ('tcp://10.31.241.47:41165',),\n",
       " 'create_cudf-5917e56cda471a44f4834546bfcac9ba': ('tcp://10.31.241.47:38489',),\n",
       " 'create_cudf-8d78d86a7729d3150eb1b8f7b0cc4c4d': ('tcp://10.31.241.47:46597',),\n",
       " 'create_cudf-96c79b3959983f80afb0e61ab1df27a1': ('tcp://10.31.241.47:35201',),\n",
       " 'create_cudf-e2fc6683845ae641dac2c0a379b81ca2': ('tcp://10.31.241.47:33516',),\n",
       " 'get_ipc_handles-28ff925961d748ead9ffcdda6781f9d9': ('tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'get_ipc_handles-3e3a0679c38607409c418eb75840639a': ('tcp://10.31.241.47:33516',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'get_ipc_handles-47071f2913142f39eb4c2d6526eb10a5': ('tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'get_ipc_handles-508189c060da94aebb9bb373d9fc86f7': ('tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:41165'),\n",
       " 'int-58e78e1b34eb49a68c65b54815d1b158': ('tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:33516'),\n",
       " 'int-5c8a950061aa331153f4a172bbcbfd1b': ('tcp://10.31.241.47:33516',\n",
       "  'tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:38489'),\n",
       " 'int-5cd9541ea58b401f115b751e79eabbff': ('tcp://10.31.241.47:33516',\n",
       "  'tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:38489'),\n",
       " 'int-c0a8a20f903a4915b94db8de3ea63195': ('tcp://10.31.241.47:35201',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:33516'),\n",
       " 'int-d3395e15f605bc35ab1bac6341a285e2': ('tcp://10.31.241.47:38489',\n",
       "  'tcp://10.31.241.47:46597',\n",
       "  'tcp://10.31.241.47:41165',\n",
       "  'tcp://10.31.241.47:33516'),\n",
       " 'join_threads-472b47d5bf108264df36737924f42dc7': ('tcp://10.31.241.47:41165',),\n",
       " 'print_it-ad2d7b657702c4c69ed553e7aed4ec4d': ('tcp://10.31.241.47:41165',),\n",
       " 'print_it-b7b5c11e4aab7376caffbc2c206410a0': ('tcp://10.31.241.47:33516',),\n",
       " 'print_it-cacdcf21d37104c8064bf6f672447736': ('tcp://10.31.241.47:46597',),\n",
       " 'print_it-d9fc044eb5952df6cad623d32726eb11': ('tcp://10.31.241.47:38489',),\n",
       " 'print_it-e8883d3a12e710d562f381deacd1f32f': ('tcp://10.31.241.47:35201',),\n",
       " 'to_gpu_matrix-07a0d737b8775c5f17a82da87b656b9e': ('tcp://10.31.241.47:46597',),\n",
       " 'to_gpu_matrix-3e66533af6962cb849705574d56a1fc6': ('tcp://10.31.241.47:35201',),\n",
       " 'to_gpu_matrix-74792332d08d267a8da00ab952de37d1': ('tcp://10.31.241.47:38489',),\n",
       " 'to_gpu_matrix-87e72d9fc96ad48003e4664b5a49714c': ('tcp://10.31.241.47:33516',),\n",
       " 'to_gpu_matrix-a815667676dd9ba17c7c2ce0bdbc2c3f': ('tcp://10.31.241.47:41165',)}"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-323-b00e420a6399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.who_has()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
