{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Mean Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "60770839-8745-4b6d-8967-015bade0ce45"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MGMean'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e54945c22219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdask_cudf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdask_cuml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/share/conda/cuml/lib/python3.5/site-packages/dask_cuml-0+untagged.26.g8eef9f7.dirty-py3.5.egg/dask_cuml/mean.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_host_port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIPCThread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_host_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcuml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMGMean\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcumlMGMean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MGMean'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "import cudf, dask_cudf\n",
    "from dask_cuml import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple ways to get data into cuml, which will need to be tested:\n",
    "1. A large cudf object could be created and then passed to dask_cudf\n",
    "2. The workers are asked to fetch the data directly\n",
    "\n",
    "Since this will likely be running in a single worker per GPU mode, it will be important that the cuDF's are able to work across the GPUs (e.g. When a very large cuDF is partitioned across the workers- it will be important that the GPU memory is re-allocated to the new worker's local device and de-allocated on the cuDF's old device.)\n",
    "\n",
    "__Example workflow__:\n",
    "- User allocates a dask_cudf (or, eventually, a dask_cuml_array) and distributes it across the cluster\n",
    "- User calls MGMean().calculate(dask_cudf) after the dask_cudf\n",
    "- MGMean performs redistribution / preprocessing\n",
    "- MGMean gathers allocations (hostname/device/key triplets) from Dask workers\n",
    "- MGMean c++ code is executed with the allocation information as its argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "ef9fe3a0-56dd-4152-9d7a-17230e76f9f2"
    }
   },
   "outputs": [],
   "source": [
    "client = Client(\"10.31.241.47:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "5d005f81-ffde-4df1-a9ab-61181f2c6748"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.31.241.47:8786\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>54.10 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.31.241.47:8786' processes=8 cores=8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cudf(dev):\n",
    "    import numba.cuda\n",
    "    import numpy as np\n",
    "    numba.cuda.select_device(dev)\n",
    "    logging.debug(\"Creating dataframe on device \" + str(dev))\n",
    "    return (dev, cudf.DataFrame(\n",
    "        [('a', np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]).astype(np.float32)), \n",
    "         ('b', np.array([2.0, 3.0, 4.0, 5.0, 6.0, 7.0]).astype(np.float32))]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcp://10.31.241.47:45359',\n",
       " 'tcp://10.31.241.47:40629',\n",
       " 'tcp://10.31.241.47:39865',\n",
       " 'tcp://10.31.241.47:42494',\n",
       " 'tcp://10.31.241.47:34624',\n",
       " 'tcp://10.31.241.47:42987',\n",
       " 'tcp://10.31.241.47:43671',\n",
       " 'tcp://10.31.241.47:46136']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers = list(client.has_what().keys())\n",
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2018, NVIDIA CORPORATION.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "from cuml import MGMean as cumlMGMean\n",
    "\n",
    "from tornado import gen\n",
    "import dask_cudf, cudf\n",
    "\n",
    "import logging\n",
    "\n",
    "import time\n",
    "\n",
    "from dask.distributed import get_worker, get_client\n",
    "\n",
    "from dask import delayed\n",
    "from collections import defaultdict\n",
    "from dask.distributed import wait, default_client\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from toolz import first, assoc\n",
    "from distributed import Client\n",
    "\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "def to_gpu_matrix(inp):\n",
    "    dev, df = inp\n",
    "    select_device(dev)\n",
    "    \n",
    "    try:\n",
    "        gpu_matrix = df.as_gpu_matrix(order='F')\n",
    "        shape = df.shape[1]\n",
    "        dtype = gpu_matrix.dtype\n",
    "        z =np.zeros(shape, dtype=dtype)\n",
    "        series = cudf.Series(z)\n",
    "        gpu_array = series._column._data.to_gpu_array()\n",
    "        return (dev, gpu_matrix, series, gpu_array)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        logging.error(\"Error in to_gpu_matrix(dev=\" + str(dev) + \"): \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        pass\n",
    "\n",
    "from threading import Thread\n",
    "import numba.cuda\n",
    "import time\n",
    "\n",
    "def select_device(dev, close = True):\n",
    "    if numba.cuda.get_current_device().id != dev:\n",
    "        logging.warn(\"Selecting device \" + str(dev))\n",
    "        if close:\n",
    "            numba.cuda.close()\n",
    "        numba.cuda.select_device(dev)\n",
    "        if dev != numba.cuda.get_current_device().id:\n",
    "            logging.warn(\"Current device \" + \n",
    "                          str(numba.cuda.get_current_device()) + \n",
    "                          \" does not match expected \" + str(dev))\n",
    "\n",
    "\n",
    "from threading import Lock\n",
    "\n",
    "class IPCThread(Thread):\n",
    "    \"\"\"\n",
    "    This mechanism gets around Numba's restriction of CUDA contexts being thread-local \n",
    "    by creating a thread that can select its own device. This allows the user of IPC \n",
    "    handles to open them up directly on the same device as the owner (bypassing the \n",
    "    need for peer access.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ipcs, device):\n",
    "        \n",
    "        Thread.__init__(self)\n",
    "\n",
    "        self.lock = Lock()\n",
    "        self.ipcs = ipcs\n",
    "        self.device = device\n",
    "        self.running = False\n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        select_device(self.device)\n",
    "        \n",
    "        print(\"Opening: \" + str(self.device) + \" \" + str(numba.cuda.get_current_device()))\n",
    "\n",
    "        self.lock.acquire()\n",
    "\n",
    "        try:\n",
    "            self.arrs = [ipc.open() for ipc in self.ipcs]\n",
    "            self.ptr_info = [x.__cuda_array_interface__ for x in self.arrs]\n",
    "\n",
    "            self.running = True\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error opening ipc_handle on device \" + str(self.device) + \": \" + str(e))\n",
    "        \n",
    "        self.lock.release()\n",
    "\n",
    "        while(self.running):\n",
    "            time.sleep(0.0001)\n",
    "            \n",
    "        try:\n",
    "            logging.warn(\"Closing: \" + str(self.device) + str(numba.cuda.get_current_device()))\n",
    "            self.lock.acquire()\n",
    "            [ipc.close() for ipc in self.ipcs]\n",
    "            self.lock.release()\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error closing ipc_handle on device \" + str(self.device) + \": \" + str(e))\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        This should be called before calling join(). Otherwise, IPC handles may not be \n",
    "        properly cleaned up. \n",
    "        \"\"\"\n",
    "        self.lock.acquire()\n",
    "        self.running = False\n",
    "        self.lock.release()\n",
    "        \n",
    "    def info(self):\n",
    "        \"\"\"\n",
    "        Warning: this method is invoked from the calling thread. Make\n",
    "        sure the context in the thread reading the memory is tied to\n",
    "        self.device, otherwise an expensive peer access might take\n",
    "        place underneath.\n",
    "        \"\"\"\n",
    "        while(not self.running):\n",
    "            time.sleep(0.0001)\n",
    "            \n",
    "        return self.ptr_info\n",
    "\n",
    "\n",
    "def build_alloc_info(data):\n",
    "    dev, gpu_matrix, series, gpu_array = data\n",
    "    return [gpu_matrix.__cuda_array_interface__, gpu_array.__cuda_array_interface__]\n",
    "\n",
    "def get_ipc_handles(data):\n",
    "    \n",
    "    dev, gpu_matrix, series, gpu_array = data\n",
    "\n",
    "    select_device(dev)\n",
    "    try:\n",
    "\n",
    "        logging.warn(\"Building in_handle on \" + str(dev))\n",
    "        in_handle = gpu_matrix.get_ipc_handle()\n",
    "        \n",
    "        logging.warn(\"Building out_handle on \" + str(dev))\n",
    "        out_handle = gpu_array.get_ipc_handle()\n",
    "        return (dev, in_handle, out_handle)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        logging.error(\"Error in get_ipc_handles(dev=\" + str(dev) + \"): \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        pass\n",
    "\n",
    "\n",
    "# Run on a single worker on each unique host\n",
    "def calc_mean(data):\n",
    "\n",
    "    ipcs, raw_arrs = data\n",
    "    \n",
    "    # Get device from local gpu_futures\n",
    "    select_device(raw_arrs[0][0])\n",
    "    \n",
    "    print(\"begin calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "\n",
    "    def new_ipc_thread(dev, ipcs):\n",
    "        t = IPCThread(ipcs, dev)\n",
    "        t.start()\n",
    "        return t\n",
    "    \n",
    "    open_ipcs = [new_ipc_thread(dev, [inp, outp]) for dev, inp, outp in ipcs]\n",
    "    logging.debug(\"calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    m = cumlMGMean()\n",
    "    \n",
    "    alloc_info = [t.info() for t in open_ipcs]\n",
    "    alloc_info.extend([build_alloc_info(t) for t in raw_arrs])\n",
    "    \n",
    "    logging.debug(\"calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    m.calculate(alloc_info)\n",
    "\n",
    "    logging.debug(\"end calc_mean_device: \" + str(numba.cuda.get_current_device()))\n",
    "    return open_ipcs, raw_arrs\n",
    "    \n",
    "class MGMean(object):\n",
    "\n",
    "    def calculate(self, futures):\n",
    "        \n",
    "        client = default_client()\n",
    "\n",
    "        # Keep the futures around so the GPU memory doesn't get\n",
    "        # deallocated on the workers.\n",
    "        gpu_futures = client.sync(self._get_mg_info, futures)\n",
    "\n",
    "        who_has = client.who_has(gpu_futures)\n",
    "\n",
    "        key_to_host_dict = {}\n",
    "        for key in who_has:\n",
    "            key_to_host_dict[key] = parse_host_port(who_has[key][0])\n",
    "            \n",
    "        hosts_to_key_dict = {}\n",
    "        for key, host in key_to_host_dict.items():\n",
    "            if host not in hosts_to_key_dict:\n",
    "                hosts_to_key_dict[host] = set([key])\n",
    "            else:\n",
    "                hosts_to_key_dict[host].add(key)\n",
    "\n",
    "        workers = [key[0] for key in list(who_has.values())]\n",
    "        hosts_dict = build_host_dict(workers)\n",
    "        f = []\n",
    "        for host, ports in hosts_dict.items():\n",
    "            exec_node = (host, random.sample(ports, 1)[0])\n",
    "            \n",
    "            logging.debug(\"Chosen exec node is \"  + str(exec_node))\n",
    "            \n",
    "            # Don't build an ipc_handle for exec nodes (we can just grab the local data)\n",
    "            keys = set(hosts_to_key_dict[exec_node])\n",
    "            \n",
    "            # build ipc handles\n",
    "            gpu_data_excl_worker = filter(lambda d: d[0] != exec_node, gpu_futures)\n",
    "            gpu_data_incl_worker = filter(lambda d: d[0] == exec_node, gpu_futures)\n",
    "            \n",
    "            ipc_handles = [client.submit(get_ipc_handles, future, workers=[worker])\n",
    "                           for worker, future in gpu_data_excl_worker]\n",
    "            raw_arrays = [future for worker, future in gpu_data_incl_worker]\n",
    "            \n",
    "            logging.debug(str(ipc_handles))\n",
    "            logging.debug(str(raw_arrays))\n",
    "            \n",
    "            f.append(client.submit(calc_mean, (ipc_handles, raw_arrays), workers = [exec_node]))\n",
    "\n",
    "        wait(f)\n",
    "        \n",
    "        def close_threads(d):\n",
    "            logging.debug(str(\"Closing threads!\"))\n",
    "            ipc_threads, rawarrays = d\n",
    "            [t.close() for t in ipc_threads]\n",
    "            \n",
    "        d = [client.submit(close_threads, future) for future in f]\n",
    "        wait(d)\n",
    "        \n",
    "        def join_threads(d):\n",
    "            logging.debug(str(\"Joining threads!\"))\n",
    "            ipc_threads, rawarrays = d\n",
    "            [t.join() for t in ipc_threads]\n",
    "            \n",
    "        d = [client.submit(join_threads, future) for future in f]\n",
    "        wait(d)\n",
    "        \n",
    "        def print_it(data):\n",
    "            dev, gpu_mat, series, gpu_arr = data\n",
    "            return str(series)\n",
    "\n",
    "        return client.gather([client.submit(print_it, future) for worker, future in gpu_futures])\n",
    "    \n",
    "\n",
    "    @gen.coroutine\n",
    "    def _get_mg_info(self, futures):\n",
    "\n",
    "        client = default_client()\n",
    "\n",
    "        if isinstance(futures, dd.DataFrame):\n",
    "            data_parts = futures.to_delayed()\n",
    "            parts = list(map(delayed, data_parts))\n",
    "            parts = client.compute(parts)  # Start computation in the background\n",
    "            yield wait(parts)\n",
    "            for part in parts:\n",
    "                if part.status == 'error':\n",
    "                    yield part  # trigger error locally\n",
    "        else:\n",
    "            data_parts = futures\n",
    "\n",
    "        key_to_part_dict = dict([(str(part.key), part) for part in data_parts])\n",
    "\n",
    "        who_has = yield client.who_has(data_parts)\n",
    "        worker_map = []\n",
    "        for key, workers in who_has.items():\n",
    "            \n",
    "            worker = parse_host_port(first(workers))\n",
    "            worker_map.append((worker, key_to_part_dict[key]))\n",
    "\n",
    "        gpu_data = [(worker, client.submit(to_gpu_matrix, part, workers=[worker]))\n",
    "                    for worker, part in worker_map]\n",
    "        \n",
    "        yield wait(gpu_data)\n",
    "\n",
    "        raise gen.Return(gpu_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "import random\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "def build_host_dict(workers):\n",
    "    hosts = set(map(lambda x: parse_host_port(x), workers))\n",
    "    hosts_dict = {}\n",
    "    for host, port in hosts:\n",
    "        if host not in hosts_dict:\n",
    "            hosts_dict[host] = set([port])\n",
    "        else:\n",
    "            hosts_dict[host].add(port)\n",
    "            \n",
    "    return hosts_dict\n",
    "    \n",
    "\n",
    "def assign_gpus():\n",
    "    \n",
    "    client = default_client()\n",
    "    \n",
    "    \"\"\"\n",
    "    Supports a multi-GPU & multi-Node environment by assigning a single local GPU\n",
    "    to each worker in the cluster. This is necessary due to Numba's restriction\n",
    "    that only a single CUDA context (and thus a single device) can be active on a \n",
    "    thread at a time. \n",
    "    \n",
    "    The GPU assignments are valid as long as the future returned from this function\n",
    "    is held in scope. This allows any functions that need to allocate GPU data to\n",
    "    utilize the CUDA context on the same device, otherwise data could be lost.\n",
    "    \"\"\"\n",
    "\n",
    "    workers = list(client.has_what().keys())\n",
    "    hosts_dict = build_host_dict(workers)\n",
    "    \n",
    "    print(str(hosts_dict))\n",
    "    \n",
    "    def get_gpu_info():\n",
    "        import numba.cuda\n",
    "        return [x.id for x in numba.cuda.gpus]\n",
    "    \n",
    "    gpu_info = dict([(host, client.submit(get_gpu_info, \n",
    "                     workers = [(host, random.sample(hosts_dict[host], 1)[0])])) \n",
    "                     for host in hosts_dict])\n",
    "    wait(list(gpu_info.values()))\n",
    "    \n",
    "    # Scatter out a GPU device ID to workers\n",
    "    f = []\n",
    "    for host, future in gpu_info.items():\n",
    "        gpu_ids = future.result()\n",
    "        ports = random.sample(hosts_dict[host], min(len(gpu_ids), len(hosts_dict[host])))\n",
    "        \n",
    "        f.extend([client.scatter(device_id, workers = [(host,port)]) for device_id, port in zip(gpu_ids, ports)])\n",
    "        \n",
    "    wait(f)\n",
    "        \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10.31.241.47': {34624, 42987, 45359, 40629, 43671, 46136, 39865, 42494}}\n"
     ]
    }
   ],
   "source": [
    "assignments = assign_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int-06e5a71c9839bd98760be56f629b24cc': ('tcp://10.31.241.47:42494',),\n",
       " 'int-58e78e1b34eb49a68c65b54815d1b158': ('tcp://10.31.241.47:34624',),\n",
       " 'int-5c8a950061aa331153f4a172bbcbfd1b': ('tcp://10.31.241.47:46136',),\n",
       " 'int-5cd9541ea58b401f115b751e79eabbff': ('tcp://10.31.241.47:43671',),\n",
       " 'int-7ec5d3339274cee5cb507a4e4d28e791': ('tcp://10.31.241.47:40629',),\n",
       " 'int-c0a8a20f903a4915b94db8de3ea63195': ('tcp://10.31.241.47:39865',),\n",
       " 'int-ce9a05dd6ec76c6a6d171b0c055f3127': ('tcp://10.31.241.47:42987',),\n",
       " 'int-d3395e15f605bc35ab1bac6341a285e2': ('tcp://10.31.241.47:45359',)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done={<Future: status: finished, type: tuple, key: create_cudf-d54089d155c6bb74354100a1920225a4>, <Future: status: finished, type: tuple, key: create_cudf-ff3f8953e808d4bdf5c6b5a626ad9ced>, <Future: status: finished, type: tuple, key: create_cudf-9f468e754fec35f10ff68e895241e6e7>, <Future: status: finished, type: tuple, key: create_cudf-ff053e7be3d0887b01fc04c312563d9c>, <Future: status: finished, type: tuple, key: create_cudf-05cc98e6e07988f1b0844cbc69e051d4>, <Future: status: finished, type: tuple, key: create_cudf-0865da5f3751794ff89a9cfd76ddab89>, <Future: status: error, key: create_cudf-47da4823f4da337847849aab9e42b2a9>, <Future: status: finished, type: tuple, key: create_cudf-cfdcd1dc4687751839df9dfba915bb21>}, not_done=set())"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [client.submit(create_cudf, future, workers = [worker]) for future, worker in zip(assignments, workers)]\n",
    "wait(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "058fd901-c613-4d64-88cb-fccc3181a269"
    }
   },
   "outputs": [],
   "source": [
    "m = MGMean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e4f48eb23ae8>\u001b[0m in \u001b[0;36m_get_mg_info\u001b[0;34m(self, futures)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_host_port\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0mworker_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_to_part_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/conda/cuml/lib/python3.5/site-packages/toolz/itertoolz.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \"\"\"\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9a11d8027ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-e4f48eb23ae8>\u001b[0m in \u001b[0;36mcalculate\u001b[0;34m(self, futures)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Keep the futures around so the GPU memory doesn't get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# deallocated on the workers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mgpu_futures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_mg_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mwho_has\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwho_has\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_futures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/conda/cuml/lib/python3.5/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/conda/cuml/lib/python3.5/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/conda/cuml/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/conda/cuml/lib/python3.5/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/conda/cuml/lib/python3.5/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhad_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/conda/cuml/lib/python3.5/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tb_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/conda/cuml/lib/python3.5/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                             \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m                         \u001b[0myielded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mstack_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontexts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0morig_stack_contexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "m.calculate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'create_cudf-01cfabc552f4f767c004c9b8dc0b092d': ('tcp://10.31.241.47:45359',),\n",
       " 'create_cudf-5b6ddb98e5e8fa9d8804bb716780e75e': ('tcp://10.31.241.47:40629',),\n",
       " 'create_cudf-97838ce0467ab5343135d1d9c4d38b51': ('tcp://10.31.241.47:42494',),\n",
       " 'create_cudf-a6be9ce53f0a0f356ed90cb9f989b1ee': ('tcp://10.31.241.47:34624',),\n",
       " 'create_cudf-bf0133384caf5a1fe2ddc94a81de3747': ('tcp://10.31.241.47:46136',),\n",
       " 'create_cudf-e525748e1f6b3bf2fe3ce65e58eb7745': ('tcp://10.31.241.47:43671',),\n",
       " 'create_cudf-f7df963bd0c86160c4fc45cb71bb62b3': ('tcp://10.31.241.47:39865',),\n",
       " 'create_cudf-fbd46cf372eecceab73318b3f5689759': ('tcp://10.31.241.47:42987',),\n",
       " 'int-06e5a71c9839bd98760be56f629b24cc': ('tcp://10.31.241.47:34624',\n",
       "  'tcp://10.31.241.47:40629'),\n",
       " 'int-58e78e1b34eb49a68c65b54815d1b158': ('tcp://10.31.241.47:39865',),\n",
       " 'int-5c8a950061aa331153f4a172bbcbfd1b': ('tcp://10.31.241.47:45359',\n",
       "  'tcp://10.31.241.47:46136'),\n",
       " 'int-5cd9541ea58b401f115b751e79eabbff': ('tcp://10.31.241.47:45359',\n",
       "  'tcp://10.31.241.47:42987'),\n",
       " 'int-7ec5d3339274cee5cb507a4e4d28e791': ('tcp://10.31.241.47:43671',),\n",
       " 'int-c0a8a20f903a4915b94db8de3ea63195': ('tcp://10.31.241.47:34624',\n",
       "  'tcp://10.31.241.47:40629'),\n",
       " 'int-ce9a05dd6ec76c6a6d171b0c055f3127': ('tcp://10.31.241.47:42987',\n",
       "  'tcp://10.31.241.47:46136'),\n",
       " 'int-d3395e15f605bc35ab1bac6341a285e2': ('tcp://10.31.241.47:42494',)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b00e420a6399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'create_cudf-0e48e7373118138593b3b44a113a85e4': ('tcp://10.31.241.47:33707',),\n",
       " 'create_cudf-1006dc4a9f499abd1299a8d5366e9419': ('tcp://10.31.241.47:37111',),\n",
       " 'create_cudf-1bce8d98ea489bc535ca6b583b152c9a': ('tcp://10.31.241.47:40989',),\n",
       " 'create_cudf-21d407fb894df1cef687d30ca9e60d10': ('tcp://10.31.241.47:33489',),\n",
       " 'create_cudf-4356fccf4b64c722f137260005ecc8d3': ('tcp://10.31.241.47:37111',),\n",
       " 'create_cudf-4574330d22f51c1a659f681a578d912c': ('tcp://10.31.241.47:37817',),\n",
       " 'create_cudf-558ba73530133f681c17863e2ce3701b': ('tcp://10.31.241.47:44554',),\n",
       " 'create_cudf-5ca6b344498c49aab2fe715a98e9c550': ('tcp://10.31.241.47:33489',),\n",
       " 'create_cudf-724f7b73a573c1763c187def2147ec4e': ('tcp://10.31.241.47:33707',),\n",
       " 'create_cudf-7bc558daec6bf3543ca793024d89c051': ('tcp://10.31.241.47:37817',),\n",
       " 'create_cudf-8082bdccc0dcd5caf09e25f03dd71037': ('tcp://10.31.241.47:35132',),\n",
       " 'create_cudf-9c17a4f904642d9c2c5a28b1518f286f': ('tcp://10.31.241.47:35132',),\n",
       " 'create_cudf-b0f39a9aafb70801d15fe47423d4472f': ('tcp://10.31.241.47:40989',),\n",
       " 'create_cudf-dd8160b4db343665b4407aaf3eb4f8af': ('tcp://10.31.241.47:44554',),\n",
       " 'int-58e78e1b34eb49a68c65b54815d1b158': ('tcp://10.31.241.47:35132',\n",
       "  'tcp://10.31.241.47:40989',\n",
       "  'tcp://10.31.241.47:44554',\n",
       "  'tcp://10.31.241.47:37111',\n",
       "  'tcp://10.31.241.47:33489',\n",
       "  'tcp://10.31.241.47:33707',\n",
       "  'tcp://10.31.241.47:37817'),\n",
       " 'int-5c8a950061aa331153f4a172bbcbfd1b': ('tcp://10.31.241.47:35132',\n",
       "  'tcp://10.31.241.47:40989',\n",
       "  'tcp://10.31.241.47:44554',\n",
       "  'tcp://10.31.241.47:37111',\n",
       "  'tcp://10.31.241.47:33489',\n",
       "  'tcp://10.31.241.47:33707',\n",
       "  'tcp://10.31.241.47:37817'),\n",
       " 'int-5cd9541ea58b401f115b751e79eabbff': ('tcp://10.31.241.47:33707',\n",
       "  'tcp://10.31.241.47:35132',\n",
       "  'tcp://10.31.241.47:40989',\n",
       "  'tcp://10.31.241.47:44554',\n",
       "  'tcp://10.31.241.47:33489',\n",
       "  'tcp://10.31.241.47:37817'),\n",
       " 'int-7ec5d3339274cee5cb507a4e4d28e791': ('tcp://10.31.241.47:35132',\n",
       "  'tcp://10.31.241.47:40989',\n",
       "  'tcp://10.31.241.47:44554',\n",
       "  'tcp://10.31.241.47:37111',\n",
       "  'tcp://10.31.241.47:33489',\n",
       "  'tcp://10.31.241.47:33707',\n",
       "  'tcp://10.31.241.47:37817'),\n",
       " 'int-c0a8a20f903a4915b94db8de3ea63195': ('tcp://10.31.241.47:35132',\n",
       "  'tcp://10.31.241.47:40989',\n",
       "  'tcp://10.31.241.47:44554',\n",
       "  'tcp://10.31.241.47:37111',\n",
       "  'tcp://10.31.241.47:33489',\n",
       "  'tcp://10.31.241.47:33707',\n",
       "  'tcp://10.31.241.47:37817'),\n",
       " 'int-ce9a05dd6ec76c6a6d171b0c055f3127': ('tcp://10.31.241.47:33707',\n",
       "  'tcp://10.31.241.47:35132',\n",
       "  'tcp://10.31.241.47:40989',\n",
       "  'tcp://10.31.241.47:37111',\n",
       "  'tcp://10.31.241.47:33489',\n",
       "  'tcp://10.31.241.47:37817'),\n",
       " 'int-d3395e15f605bc35ab1bac6341a285e2': ('tcp://10.31.241.47:35132',\n",
       "  'tcp://10.31.241.47:40989',\n",
       "  'tcp://10.31.241.47:44554',\n",
       "  'tcp://10.31.241.47:37111',\n",
       "  'tcp://10.31.241.47:33489',\n",
       "  'tcp://10.31.241.47:37817')}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
